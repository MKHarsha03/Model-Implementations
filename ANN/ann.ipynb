{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights initialization\n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    for i in range(len(1,len(layer_dims))):\n",
    "        parameters['W' + str(i)] = np.random.randn((layer_dims[i],layer_dims[i-1])) * 0.01\n",
    "        parameters['b' + str(i)] = np.random.randn((layer_dims[i],1)) * 0.01\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activations and their derivatives\n",
    "def linear(W,x,b):\n",
    "    return np.dot(W,x) + b\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "def sigmoid_der(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def relu_der(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z,axis=1,keepdims=True))\n",
    "    return exp_z / np.sum(exp_z,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function and derivative\n",
    "def cross_entropy_loss(y_true,y_pred):\n",
    "    n_samples = y_true.shape[0]\n",
    "    y_pred = np.clip(y_pred,1e-12,1-1e-12)\n",
    "    return -np.sum(y_true*np.log(y_pred))/n_samples\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true,y_pred):\n",
    "    return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propagation single iteration:\n",
    "def forward(A_prev,W,b,activation):\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "\n",
    "    if activation == 'linear':\n",
    "        return Z\n",
    "    elif activation == 'sigmoid':\n",
    "        return sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        return relu(Z)\n",
    "    elif activation == 'softmax':\n",
    "        return softmax(Z)\n",
    "\n",
    "\n",
    "#Forward propagation along all layers once\n",
    "def forward_propagation(A,parameters,activations):\n",
    "    for i in range(len(activations)-1):\n",
    "        parameters['A'+str(i)] = forward(A,parameters['W'+str(i+1)],parameters['b'+str(i+1)],activations[i])\n",
    "    \n",
    "    y_pred = parameters['A' + str(len(activations)-1)]\n",
    "    return y_pred,parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation single iteration\n",
    "def backward(dA, Z, A_prev, W, activation):\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    if activation == 'relu':\n",
    "        dZ = dA * relu_der(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = dA * sigmoid_der(Z)\n",
    "    elif activation == 'linear':\n",
    "        dZ = dA\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "# Backward propagation along all layers\n",
    "def backward_propagation(y_true, y_pred, parameters, activations):\n",
    "    grads = {}\n",
    "    m = y_true.shape[1]\n",
    "    L = len(activations) - 1  # number of layers\n",
    "\n",
    "    # Derivative of the loss with respect to the output (softmax layer)\n",
    "    dA = cross_entropy_loss_derivative(y_true, y_pred)\n",
    "\n",
    "    for l in reversed(range(L)):\n",
    "        dA, dW, db = backward(dA, parameters['A'+str(l)], parameters['A'+str(l-1)], parameters['W'+str(l+1)], activations[l])\n",
    "        grads['dW'+str(l+1)] = dW\n",
    "        grads['db'+str(l+1)] = db\n",
    "\n",
    "    return grads\n",
    "\n",
    "#Update parameters using gradient descent\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] -= learning_rate * grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] -= learning_rate * grads['db'+str(l+1)]\n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, layer_dims, activations, epochs, learning_rate):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation\n",
    "        y_pred, parameters = forward_propagation(X, parameters, activations)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(y, y_pred)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(y, y_pred, parameters, activations)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Load and preprocess the MNIST dataset\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_mnist_data\u001b[39m():\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "# Load and preprocess the MNIST dataset\n",
    "def load_mnist_data():\n",
    "    # Load the dataset\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Flatten the images from 28x28 to 784 and normalize pixel values to [0, 1]\n",
    "    X_train = X_train.reshape(-1, 784).T / 255.0  # Shape: (784, n_train_samples)\n",
    "    X_test = X_test.reshape(-1, 784).T / 255.0    # Shape: (784, n_test_samples)\n",
    "    \n",
    "    # Convert labels to one-hot encoding if needed\n",
    "    y_train_one_hot = np.eye(10)[y_train].T  # Shape: (10, n_train_samples)\n",
    "    y_test_one_hot = np.eye(10)[y_test].T    # Shape: (10, n_test_samples)\n",
    "    \n",
    "    # Return data with integer labels or one-hot encoded labels\n",
    "    return X_train, y_train_one_hot, X_test, y_test_one_hot\n",
    "\n",
    "def predict(X, parameters):\n",
    "    y_pred, _ = forward_propagation(X, parameters)\n",
    "    predictions = np.argmax(y_pred, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
